Deep Compression : Compressing deep neural networks with pruning, trained quantization and Huffman coding

1. Introduction
 1.1 reduce storage and energy required to runinference
 1.2 three strategy
  1) model pruning : 많은 정보를 포함하는 connection만을 유지한채로 쓸모없는 connection을 제거
  2) quantization : 비슷한 값을 가진 weight들을 동일한 weight 갖도록 bin에 넣어 weitht sharing을 한 후 bin의 인덱스만을 저장
  3) huffman coding : 가장 빈번한 weight 값을 적은 bit로 표현

2. Network Pruning
 small connection removing (unstructured pruning)

3. Quantization and Weight Sharing
 3.1 weight sharing : quantization 을 함으로써 정수 index 로 표현.
 3.2 8 bit 으로 quantize 한다는 것은 2^8, 즉 256 개의 index 로 표현함을 의미한다.
     이 방식을 통해서 Alexnet 은 conv layer 은 8bit, linear layer 은 5 bit 로 quantization 을 해주게 된다.
     compression rate 는 (input bit/output bit) 를 통해서 구해줄 수 있다.
 3.3 weight sharing

4. Huffman Coding
 4.1
 4.2
